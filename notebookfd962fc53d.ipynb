{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:51:53.221900Z","iopub.execute_input":"2024-11-11T15:51:53.222347Z","iopub.status.idle":"2024-11-11T15:51:54.654275Z","shell.execute_reply.started":"2024-11-11T15:51:53.222302Z","shell.execute_reply":"2024-11-11T15:51:54.653031Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%pip install -U -q \"google-generativeai>=0.8.3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:53:09.346315Z","iopub.execute_input":"2024-11-11T15:53:09.346920Z","iopub.status.idle":"2024-11-11T15:53:49.212030Z","shell.execute_reply.started":"2024-11-11T15:53:09.346873Z","shell.execute_reply":"2024-11-11T15:53:49.210523Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import google.generativeai as genai\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:06:45.776426Z","iopub.execute_input":"2024-11-11T16:06:45.776915Z","iopub.status.idle":"2024-11-11T16:06:45.783526Z","shell.execute_reply.started":"2024-11-11T16:06:45.776870Z","shell.execute_reply":"2024-11-11T16:06:45.781954Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:57:28.192159Z","iopub.execute_input":"2024-11-11T15:57:28.192761Z","iopub.status.idle":"2024-11-11T15:57:28.540049Z","shell.execute_reply.started":"2024-11-11T15:57:28.192701Z","shell.execute_reply":"2024-11-11T15:57:28.538783Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"flash = genai.GenerativeModel('gemini-1.5-flash')\nresponse = flash.generate_content(\"Explain AI to me like I'm a kid.\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:57:59.331227Z","iopub.execute_input":"2024-11-11T15:57:59.331762Z","iopub.status.idle":"2024-11-11T15:58:01.563786Z","shell.execute_reply.started":"2024-11-11T15:57:59.331717Z","shell.execute_reply":"2024-11-11T15:58:01.562398Z"}},"outputs":[{"name":"stdout","text":"Imagine you have a super smart friend who loves to learn and solve problems. That friend is like AI, or Artificial Intelligence.  \n\nAI is like a computer program that can learn and think like a human. It can do lots of cool things, like:\n\n* **Play games**: AI can play chess, video games, and even beat the best human players!\n* **Help you with your homework**: It can answer your questions, translate languages, and even write stories!\n* **Make your life easier**: AI can help you find information online, order food, and even control your lights and TV!\n\nAI learns by looking at lots of information, just like you learn by reading books or watching videos. It then uses that information to solve problems and make decisions.  \n\nThink of it like learning to ride a bike. At first, you might fall down a lot, but with practice, you get better and better. AI is like that - it gets smarter and more helpful over time!\n\nThere are many different types of AI, but they all have one thing in common: they want to help us make the world a better place! \n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:58:15.666541Z","iopub.execute_input":"2024-11-11T15:58:15.666979Z","iopub.status.idle":"2024-11-11T15:58:15.677197Z","shell.execute_reply.started":"2024-11-11T15:58:15.666938Z","shell.execute_reply":"2024-11-11T15:58:15.675907Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Imagine you have a super smart friend who loves to learn and solve problems. That friend is like AI, or Artificial Intelligence.  \n\nAI is like a computer program that can learn and think like a human. It can do lots of cool things, like:\n\n* **Play games**: AI can play chess, video games, and even beat the best human players!\n* **Help you with your homework**: It can answer your questions, translate languages, and even write stories!\n* **Make your life easier**: AI can help you find information online, order food, and even control your lights and TV!\n\nAI learns by looking at lots of information, just like you learn by reading books or watching videos. It then uses that information to solve problems and make decisions.  \n\nThink of it like learning to ride a bike. At first, you might fall down a lot, but with practice, you get better and better. AI is like that - it gets smarter and more helpful over time!\n\nThere are many different types of AI, but they all have one thing in common: they want to help us make the world a better place! \n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"chat = flash.start_chat(history=[])\nresponse = chat.send_message('Hello! My name is Zlork.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:58:31.656501Z","iopub.execute_input":"2024-11-11T15:58:31.657288Z","iopub.status.idle":"2024-11-11T15:58:32.774559Z","shell.execute_reply.started":"2024-11-11T15:58:31.657216Z","shell.execute_reply":"2024-11-11T15:58:32.773306Z"}},"outputs":[{"name":"stdout","text":"Hello Zlork! It's nice to meet you. ðŸ˜Š  What can I do for you today? \n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"response = chat.send_message('Can you tell something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:58:44.145512Z","iopub.execute_input":"2024-11-11T15:58:44.145968Z","iopub.status.idle":"2024-11-11T15:58:45.936586Z","shell.execute_reply.started":"2024-11-11T15:58:44.145925Z","shell.execute_reply":"2024-11-11T15:58:45.935117Z"}},"outputs":[{"name":"stdout","text":"Of course! Here's something fascinating about dinosaurs: \n\n**Did you know some dinosaurs had feathers?**\n\nThat's right! While we often picture dinosaurs as scaly behemoths, many species, especially those closely related to birds, had feathers.  These feathers weren't always for flight, but could have been used for insulation, display, or even to help with running. Some of the most famous feathered dinosaurs include Velociraptor, Microraptor, and the iconic Archaeopteryx, which is considered a transitional fossil between dinosaurs and birds.\n\nThis discovery has revolutionized our understanding of dinosaurs and their evolution, showing that the line between dinosaurs and birds isn't as clear-cut as we once thought. \n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# While you have the `chat` object around, the conversation state\n# persists. Confirm that by asking if it knows my name.\nresponse = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:59:00.133194Z","iopub.execute_input":"2024-11-11T15:59:00.133655Z","iopub.status.idle":"2024-11-11T15:59:01.244649Z","shell.execute_reply.started":"2024-11-11T15:59:00.133609Z","shell.execute_reply":"2024-11-11T15:59:01.243207Z"}},"outputs":[{"name":"stdout","text":"Of course! You're Zlork. ðŸ˜„ It's nice to chat with you. \n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"for model in genai.list_models():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:59:32.556974Z","iopub.execute_input":"2024-11-11T15:59:32.557525Z","iopub.status.idle":"2024-11-11T15:59:33.458056Z","shell.execute_reply.started":"2024-11-11T15:59:32.557477Z","shell.execute_reply":"2024-11-11T15:59:33.456884Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-latest\nmodels/gemini-1.0-pro\nmodels/gemini-pro\nmodels/gemini-1.0-pro-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-pro-exp-0801\nmodels/gemini-1.5-pro-exp-0827\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-exp-0827\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/aqa\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"for model in genai.list_models():\n  if model.name == 'models/gemini-1.5-flash':\n    print(model)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T15:59:47.459940Z","iopub.execute_input":"2024-11-11T15:59:47.460379Z","iopub.status.idle":"2024-11-11T15:59:48.251954Z","shell.execute_reply.started":"2024-11-11T15:59:47.460339Z","shell.execute_reply":"2024-11-11T15:59:48.250677Z"}},"outputs":[{"name":"stdout","text":"Model(name='models/gemini-1.5-flash',\n      base_model_id='',\n      version='001',\n      display_name='Gemini 1.5 Flash',\n      description='Fast and versatile multimodal model for scaling across diverse tasks',\n      input_token_limit=1000000,\n      output_token_limit=8192,\n      supported_generation_methods=['generateContent', 'countTokens'],\n      temperature=1.0,\n      max_temperature=2.0,\n      top_p=0.95,\n      top_k=40)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"short_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(max_output_tokens=200))\n\nresponse = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:00:03.766992Z","iopub.execute_input":"2024-11-11T16:00:03.767455Z","iopub.status.idle":"2024-11-11T16:00:05.895026Z","shell.execute_reply.started":"2024-11-11T16:00:03.767403Z","shell.execute_reply":"2024-11-11T16:00:05.893718Z"}},"outputs":[{"name":"stdout","text":"## Beyond the Brine: The Enduring Importance of Olives in Modern Society\n\nThe humble olive, a small, fleshy fruit that grows on evergreen trees native to the Mediterranean, holds a place of immense cultural and economic significance in modern society. Far beyond its simple appearance lies a story woven with history, tradition, and a surprising array of benefits. From culinary delights to health-enhancing properties, the olive has permeated our lives in ways that extend far beyond the realm of mere sustenance.\n\n**A Culinary Staple with Global Reach:**\n\nThe olive has been a culinary staple for millennia, its journey from ancient civilizations to contemporary kitchens a testament to its versatility. The diverse ways in which olives are consumed - as table olives, olive oil, and even pickled - reflects their adaptability. From the classic Mediterranean diet to the modern fusion cuisine, olives provide a unique flavor profile, a savory richness that adds depth and complexity to dishes.\n\nOlive oil, extracted from the fruit, is celebrated for its culinary prowess.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:00:21.306400Z","iopub.execute_input":"2024-11-11T16:00:21.306883Z","iopub.status.idle":"2024-11-11T16:00:22.621224Z","shell.execute_reply.started":"2024-11-11T16:00:21.306837Z","shell.execute_reply":"2024-11-11T16:00:22.619974Z"}},"outputs":[{"name":"stdout","text":"From ancient groves, a fruit so green,\nA taste of history, a vibrant scene.\nThe olive, small, yet packed with might,\nA staple food, a culinary light.\n\nIn salads bright, it adds its zest,\nIn savory dishes, it's truly blessed.\nFrom oil to brine, it plays its role,\nA flavor treasured, story to be told.\n\nA symbol of peace, a gift of grace,\nA taste of sunshine, in every place.\nThe olive thrives, a modern boon,\nNourishing bodies, under sun and moon. \n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        # These are the default values for gemini-1.5-flash-001.\n        temperature=1.0,\n        top_k=64,\n        top_p=0.95,\n    ))\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = model.generate_content(story_prompt)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:12:15.175970Z","iopub.execute_input":"2024-11-11T16:12:15.176431Z","iopub.status.idle":"2024-11-11T16:12:18.420378Z","shell.execute_reply.started":"2024-11-11T16:12:15.176386Z","shell.execute_reply":"2024-11-11T16:12:18.419173Z"}},"outputs":[{"name":"stdout","text":"Bartholomew, a ginger tabby with a penchant for mischief, was bored. The usual routine of napping in sunbeams and chasing dust motes held no allure today. He yearned for something more, something exciting. As he surveyed his domain, his eye fell upon the open window. A flicker of adventure sparked in his green eyes.\n\nHe padded across the windowsill, a thrill coursing through him. The world outside beckoned, a tapestry of smells and sounds. With a flick of his tail, he leaped, landing on the window ledge outside.\n\nThe world was a riot of colors and textures. Birds sang in the trees, a squirrel chattered from a branch, and the scent of damp earth filled his nostrils. Bartholomew, emboldened, ventured further, his paws treading softly on the grass.\n\nHe soon found himself in a garden teeming with life. A fat, yellow butterfly fluttered by, its wings a kaleidoscope of patterns. He chased it, his instincts kicking in, only to be thwarted by the butterflyâ€™s swiftness. He stalked a plump, red robin, his tail twitching with anticipation, but the bird flew away before he could pounce.\n\nSuddenly, a rustling in the bushes caught his attention. He crept closer, his whiskers twitching. A small, gray kitten, its eyes wide with fear, peered at him. Bartholomew, despite his adventurous spirit, felt a pang of sympathy. The kitten, he realised, was lost.\n\nHe nudged the kitten gently, a low purr rumbling in his chest. The kitten, reassured, rubbed against his leg. He led the kitten through the garden, navigating the maze of flowerbeds and trees.\n\nAs they approached the house, the kitten suddenly stopped, its eyes wide with terror. A large, menacing dog, with a deep bark, stood at the gate. Bartholomew, without a second thought, stood protectively in front of the kitten, hissing loudly.\n\nThe dog, surprised by the felineâ€™s bravery, backed down. Bartholomew, his heart pounding, saw the kittenâ€™s eyes light up with gratitude. He had saved the day.\n\nThe kitten's human, a young girl with braids, appeared, her eyes filled with relief. She scooped up the kitten, showering it with affection. As she turned to thank Bartholomew, he gave her a gentle rub against her legs, then turned and sauntered back home, his chest puffed with pride.\n\nHe had faced his fear, gone on an adventure, and made a friend. He had saved the kitten, proving to himself that even a pampered house cat could be brave and kind.  And as he curled up in his favorite sunbeam, a satisfied purr escaped his throat. Bartholomew knew, in his heart, that this wasn't the last adventure he'd have. \n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=5,\n    ))\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = model.generate_content(zero_shot_prompt)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:12:41.381142Z","iopub.execute_input":"2024-11-11T16:12:41.382266Z","iopub.status.idle":"2024-11-11T16:12:42.423298Z","shell.execute_reply.started":"2024-11-11T16:12:41.382210Z","shell.execute_reply":"2024-11-11T16:12:42.421913Z"}},"outputs":[{"name":"stdout","text":"Sentiment: **POSITIVE**\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ))\n\nresponse = model.generate_content(zero_shot_prompt)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:12:54.696079Z","iopub.execute_input":"2024-11-11T16:12:54.696514Z","iopub.status.idle":"2024-11-11T16:12:55.381523Z","shell.execute_reply.started":"2024-11-11T16:12:54.696468Z","shell.execute_reply":"2024-11-11T16:12:55.380235Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ))\n\nfew_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\n\nresponse = model.generate_content([few_shot_prompt, customer_order])\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:13:14.961675Z","iopub.execute_input":"2024-11-11T16:13:14.962171Z","iopub.status.idle":"2024-11-11T16:13:15.490355Z","shell.execute_reply.started":"2024-11-11T16:13:14.962113Z","shell.execute_reply":"2024-11-11T16:13:15.489125Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"pineapple\"]\n}\n``` \n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ))\n\nresponse = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:13:30.182094Z","iopub.execute_input":"2024-11-11T16:13:30.182608Z","iopub.status.idle":"2024-11-11T16:13:30.754925Z","shell.execute_reply.started":"2024-11-11T16:13:30.182563Z","shell.execute_reply":"2024-11-11T16:13:30.753485Z"}},"outputs":[{"name":"stdout","text":"{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nresponse = model.generate_content(prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:13:46.389008Z","iopub.execute_input":"2024-11-11T16:13:46.389442Z","iopub.status.idle":"2024-11-11T16:13:46.885223Z","shell.execute_reply.started":"2024-11-11T16:13:46.389402Z","shell.execute_reply":"2024-11-11T16:13:46.883898Z"}},"outputs":[{"name":"stdout","text":"39 \n\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = model.generate_content(prompt)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:14:00.015982Z","iopub.execute_input":"2024-11-11T16:14:00.016483Z","iopub.status.idle":"2024-11-11T16:14:01.033794Z","shell.execute_reply.started":"2024-11-11T16:14:00.016430Z","shell.execute_reply":"2024-11-11T16:14:01.032583Z"}},"outputs":[{"name":"stdout","text":"Here's how we can solve this:\n\n* **When you were 4, your partner was 3 times your age:**  3 * 4 = 12 years old.\n* **The age difference between you and your partner:** 12 - 4 = 8 years.\n* **Since the age difference remains the same, your partner is still 8 years older than you.**\n* **Therefore, if you are now 20, your partner is:** 20 + 8 = 28 years old. \n\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:14:27.917740Z","iopub.execute_input":"2024-11-11T16:14:27.918783Z","iopub.status.idle":"2024-11-11T16:14:27.928203Z","shell.execute_reply.started":"2024-11-11T16:14:27.918731Z","shell.execute_reply":"2024-11-11T16:14:27.926831Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nreact_chat = model.start_chat()\n\n# You will perform the Action, so generate up to, but not including, the Observation.\nconfig = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n\nresp = react_chat.send_message(\n    [model_instructions, example1, example2, question],\n    generation_config=config)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:14:41.136281Z","iopub.execute_input":"2024-11-11T16:14:41.136737Z","iopub.status.idle":"2024-11-11T16:14:44.047979Z","shell.execute_reply.started":"2024-11-11T16:14:41.136693Z","shell.execute_reply":"2024-11-11T16:14:44.046686Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the Transformers paper and then look for the authors.\n\nAction 1\n<search>Transformers paper</search>\n\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation, generation_config=config)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:14:56.590871Z","iopub.execute_input":"2024-11-11T16:14:56.591953Z","iopub.status.idle":"2024-11-11T16:14:57.345210Z","shell.execute_reply.started":"2024-11-11T16:14:56.591887Z","shell.execute_reply":"2024-11-11T16:14:57.344143Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nThe observation gives the authors of the paper, but not their ages. So I need to search the authors one by one to find their age and identify the youngest.\n\nAction 2\n<search>Ashish Vaswani age</search> \n\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ))\n\n# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = model.generate_content(code_prompt)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:15:11.826627Z","iopub.execute_input":"2024-11-11T16:15:11.827068Z","iopub.status.idle":"2024-11-11T16:15:12.440560Z","shell.execute_reply.started":"2024-11-11T16:15:11.827029Z","shell.execute_reply":"2024-11-11T16:15:12.439170Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    tools='code_execution')\n\ncode_exec_prompt = \"\"\"\nCalculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you get them all.\n\"\"\"\n\nresponse = model.generate_content(code_exec_prompt)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:15:25.580793Z","iopub.execute_input":"2024-11-11T16:15:25.581353Z","iopub.status.idle":"2024-11-11T16:15:30.629146Z","shell.execute_reply.started":"2024-11-11T16:15:25.581308Z","shell.execute_reply":"2024-11-11T16:15:30.627902Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n``` python\nimport sympy\n\nprimes = sympy.primerange(1, 100)\nprimes = [p for p in primes if p % 2 != 0]\nprimes = primes[:14]\nprint(f'primes={primes}')\nprint(f'sum={sum(primes)}')\n\n```\n```\nprimes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum=326\n\n```\nThe sum of the first 14 odd prime numbers is 326. \n"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\n\nresponse = model.generate_content(explain_prompt)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:16:28.946680Z","iopub.execute_input":"2024-11-11T16:16:28.947187Z","iopub.status.idle":"2024-11-11T16:16:32.151005Z","shell.execute_reply.started":"2024-11-11T16:16:28.947142Z","shell.execute_reply":"2024-11-11T16:16:32.149853Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file, `git-prompt.sh`,  is a Bash script designed to enhance your command-line prompt with information about your current Git repository. \n\n**Here's a breakdown of what it does at a high level:**\n\n* **Provides Git-aware prompt information:** The script dynamically updates your prompt to display details like:\n    * Current branch name\n    * Number of uncommitted changes\n    * Status of upstream tracking (ahead/behind)\n    * Whether you are in a detached HEAD state\n\n* **Customization:** The script supports extensive customization with a wide range of settings, including:\n    * **Color themes:** You can choose from predefined themes or create your own custom theme.\n    * **Symbols:** Define symbols to represent different Git states (e.g., ahead, behind, untracked).\n    * **Prompt formatting:**  You can control the placement and formatting of various elements within your prompt.\n\n* **Flexibility:** \n    * The script can be used inside or outside of a Git repository, allowing you to customize your prompt even when not working on Git projects.\n    * It also integrates with virtual environments, displaying the active virtual environment name within your prompt.\n\n* **Installation:**  The script is usually installed in a directory like `~/.bashrc` or `~/.zshrc`, which allows it to be executed every time you start a new terminal session.\n\n**Why you would use it:**\n\n* **Improved workflow:**  The script provides a clear and concise overview of your Git repository's status directly in your prompt, making it easier to track your work and navigate Git operations.\n* **Enhanced awareness:** By displaying key Git information, the script helps you stay informed about your project's state without constantly switching between your prompt and Git commands.\n* **Increased efficiency:** With the relevant information readily available in your prompt, you can quickly make decisions and execute the appropriate Git commands.\n\nEssentially, this script transforms your basic command-line prompt into a powerful and informative tool for managing your Git projects. \n"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}